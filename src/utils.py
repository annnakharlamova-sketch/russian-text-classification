"""
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
"""
import datetime
import os
import yaml
import pandas as pd
import random
import numpy as np
import torch
from sklearn.model_selection import StratifiedKFold
from sklearn.utils import resample
import scipy.stats as stats

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è seed
GLOBAL_SEED = 42


def set_global_seed(seed=42):
    """
    –§–∏–∫—Å–∞—Ü–∏—è random seed –¥–ª—è –ø–æ–ª–Ω–æ–π –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    
    Args:
        seed (int): Random seed (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 42)
    """
    global GLOBAL_SEED
    GLOBAL_SEED = seed
    
    # Python random
    random.seed(seed)
    
    # NumPy
    np.random.seed(seed)
    
    # PyTorch
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è multi-GPU
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è CuDNN
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    print(f" Random seed —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≥–ª–æ–±–∞–ª—å–Ω–æ: {seed}")


def get_global_seed():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ global seed"""
    return GLOBAL_SEED


def setup_dataloader_seed(dataloader, seed=None):
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ seed –¥–ª—è DataLoader –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Args:
        dataloader: PyTorch DataLoader
        seed (int): Seed (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π)
    """
    if seed is None:
        seed = GLOBAL_SEED
    
    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)
    
    generator = torch.Generator()
    generator.manual_seed(seed)
    
    dataloader.worker_init_fn = seed_worker
    dataloader.generator = generator
    
    return dataloader


def create_stratified_cv(n_splits=5, shuffle=True, random_state=None):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ 5-–∫—Ä–∞—Ç–Ω–æ–≥–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
    
    Args:
        n_splits (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)
        shuffle (bool): –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
        random_state (int): Random seed (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π)
    
    Returns:
        StratifiedKFold: –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ç–æ—Ä
    """
    if random_state is None:
        random_state = GLOBAL_SEED
    
    cv = StratifiedKFold(
        n_splits=n_splits,
        shuffle=shuffle,
        random_state=random_state
    )
    
    print(f" –°–æ–∑–¥–∞–Ω {n_splits}-–∫—Ä–∞—Ç–Ω—ã–π —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CV:")
    print(f"   - shuffle: {shuffle}")
    print(f"   - random_state: {random_state}")
    
    return cv


def calculate_bootstrap_ci(scores, n_bootstrap=1000, confidence=0.95, random_state=None):
    """
    –†–∞—Å—á–µ—Ç 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –º–µ—Ç–æ–¥–æ–º –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å–Ω–æ–≥–æ –±—É—Ç—Å—Ç—Ä—ç–ø–∞
    
    Args:
        scores (array-like): –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫/–º–µ—Ç—Ä–∏–∫
        n_bootstrap (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É—Ç—Å—Ç—Ä—ç–ø-–≤—ã–±–æ—Ä–æ–∫
        confidence (float): –£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è (0.95 –¥–ª—è 95% CI)
        random_state (int): Random seed
    
    Returns:
        tuple: (lower_bound, upper_bound, bootstrap_samples)
    """
    if random_state is None:
        random_state = GLOBAL_SEED
    
    np.random.seed(random_state)
    
    bootstrap_samples = []
    n_samples = len(scores)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±—É—Ç—Å—Ç—Ä—ç–ø-–≤—ã–±–æ—Ä–æ–∫
    for _ in range(n_bootstrap):
        bootstrap_sample = resample(scores, replace=True, n_samples=n_samples)
        bootstrap_samples.append(np.mean(bootstrap_sample))
    
    bootstrap_samples = np.array(bootstrap_samples)
    
    # –†–∞—Å—á–µ—Ç –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–µ–π
    alpha = (1 - confidence) / 2
    lower_bound = np.percentile(bootstrap_samples, alpha * 100)
    upper_bound = np.percentile(bootstrap_samples, (1 - alpha) * 100)
    
    print(f" –†–∞—Å—Å—á–∏—Ç–∞–Ω {confidence*100}% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–±—É—Ç—Å—Ç—Ä—ç–ø):")
    print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {n_bootstrap}")
    print(f"   - –ú–µ—Ç–æ–¥: –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å–Ω—ã–π –±—É—Ç—Å—Ç—Ä—ç–ø")
    print(f"   - –î–∏–∞–ø–∞–∑–æ–Ω: [{lower_bound:.4f}, {upper_bound:.4f}]")
    print(f"   - –°—Ä–µ–¥–Ω–µ–µ: {np.mean(scores):.4f} ¬± {np.std(scores):.4f}")
    
    return lower_bound, upper_bound, bootstrap_samples


def perform_cross_validation(model, X, y, cv=None, scoring_func=None):
    """
    –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
    
    Args:
        model: –ú–æ–¥–µ–ª—å —Å fit/predict –º–µ—Ç–æ–¥–∞–º–∏
        X: –ü—Ä–∏–∑–Ω–∞–∫–∏
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        cv: –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ç–æ—Ä (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
        scoring_func: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã CV
    """
    if cv is None:
        cv = create_stratified_cv()
    
    if scoring_func is None:
        from sklearn.metrics import accuracy_score
        scoring_func = accuracy_score
    
    cv_scores = []
    fold_details = []
    
    print(" –ó–∞–ø—É—Å–∫ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    
    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞
        y_pred = model.predict(X_val)
        score = scoring_func(y_val, y_pred)
        
        cv_scores.append(score)
        fold_details.append({
            'fold': fold,
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'score': score
        })
        
        print(f"   Fold {fold}: score = {score:.4f}, "
              f"train/val = {len(train_idx)}/{len(val_idx)}")
    
    # –†–∞—Å—á–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
    lower_ci, upper_ci, bootstrap_samples = calculate_bootstrap_ci(cv_scores)
    
    results = {
        'cv_scores': cv_scores,
        'mean_score': np.mean(cv_scores),
        'std_score': np.std(cv_scores),
        'bootstrap_ci': (lower_ci, upper_ci),
        'bootstrap_samples': bootstrap_samples,
        'fold_details': fold_details,
        'cv_params': {
            'n_splits': cv.n_splits,
            'shuffle': cv.shuffle,
            'random_state': cv.random_state
        }
    }
    
    print(f" –†–µ–∑—É–ª—å—Ç–∞—Ç—ã CV:")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ: {results['mean_score']:.4f} ¬± {results['std_score']:.4f}")
    print(f"   95% CI: [{results['bootstrap_ci'][0]:.4f}, {results['bootstrap_ci'][1]:.4f}]")
    
    return results


def load_config(config_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML —Ñ–∞–π–ª–∞"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def ensure_dir(directory):
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")
    return directory


def print_config_summary(config):
    """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    print("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:")
    print(f"   Random seed: {GLOBAL_SEED}")
    print(f"   –ö–æ—Ä–ø—É—Å—ã: {list(config['data']['corpora'].keys())}")
    print(f"   –ü–∞–π–ø–ª–∞–π–Ω—ã: {list(config['preprocessing']['pipelines'].keys())}")
    print(f"   –ú–æ–¥–µ–ª–∏: {list(config['models']['classical'].keys())} + LSTM")
    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {config['evaluation']['metrics']}")


def setup_reproducibility(seed=42):
    """
    –ü–æ–ª–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ (–æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)
    
    Args:
        seed (int): Random seed
    """
    set_global_seed(seed)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è NumPy
    np.set_printoptions(precision=8, suppress=True)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è PyTorch
    torch.set_printoptions(precision=8)
    
    print(" –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞:")
    print(f"   - Python random: {seed}")
    print(f"   - NumPy: {seed}")
    print(f"   - PyTorch: {seed}")
    print(f"   - CuDNN deterministic: True")


def test_reproducibility():
    """–¢–µ—Å—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏"""
    print(" –¢–µ—Å—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏...")
    
    setup_reproducibility(42)
    
    # –¢–µ—Å—Ç NumPy
    numpy_test = np.random.rand(3)
    print(f"NumPy random: {numpy_test}")
    
    # –¢–µ—Å—Ç PyTorch
    torch_test = torch.rand(3)
    print(f"PyTorch random: {torch_test}")
    
    # –¢–µ—Å—Ç Python random
    python_test = [random.random() for _ in range(3)]
    print(f"Python random: {python_test}")
    
    print(" –¢–µ—Å—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")

def save_experiment_results(results_dict, filename=None, results_dir="results"):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CSV
    
    Args:
        results_dict (dict): –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        filename (str): –ò–º—è —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ None, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        results_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Returns:
        str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    ensure_dir(results_dir)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    required_columns = [
        'dataset', 'model', 'preprocess', 'fold', 'seed', 
        'accuracy', 'macro_f1', 'precision', 'recall', 'train_time_sec'
    ]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    for col in ['dataset', 'model', 'preprocess', 'seed']:
        if col not in results_dict:
            raise ValueError(f"–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {col}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏
    result_row = {}
    for col in required_columns:
        result_row[col] = results_dict.get(col, None)
    
    df = pd.DataFrame([result_row])
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{results_dict['dataset']}_{results_dict['model']}_{results_dict['preprocess']}_seed{results_dict['seed']}.csv"
    
    filepath = os.path.join(results_dir, filename)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–¥–æ–∑–∞–ø–∏—Å—å –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
    if os.path.exists(filepath):
        df.to_csv(filepath, mode='a', header=False, index=False, encoding='utf-8')
    else:
        df.to_csv(filepath, index=False, encoding='utf-8')
    
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
    return filepath


def load_all_results(results_dir="results"):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü
    
    Args:
        results_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    
    Returns:
        pd.DataFrame: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    all_results = []
    
    if not os.path.exists(results_dir):
        print(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {results_dir}")
        return pd.DataFrame()
    
    for file in os.listdir(results_dir):
        if file.endswith('.csv'):
            filepath = os.path.join(results_dir, file)
            try:
                df = pd.read_csv(filepath, encoding='utf-8')
                all_results.append(df)
                print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {file} ({len(df)} —Å—Ç—Ä–æ–∫)")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file}: {e}")
    
    if all_results:
        combined_df = pd.concat(all_results, ignore_index=True)
        print(f"üìà –í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(combined_df)} —Å—Ç—Ä–æ–∫")
        return combined_df
    else:
        print("üì≠ –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        return pd.DataFrame()


if __name__ == "__main__":
    test_reproducibility()