# Конфигурация LSTM модели
# Соответствует параметрам из статьи

model:
  name: "lstm"
  type: "neural"
  description: "LSTM нейросетевая модель с эмбеддингами"

architecture:
  type: "bilstm"
  parameters:
    embedding_dim: 200            # Размерность эмбеддингов
    hidden_size: 128              # Размер скрытого слоя LSTM
    num_layers: 1                 # Количество LSTM слоев
    bidirectional: true           # Двунаправленная LSTM
    dropout: 0.3                  # Dropout для регуляризации
    num_classes: 2                # Количество классов (бинарная)
    padding_idx: 0                # Индекс паддинга
    vocab_size: 50000             # Максимальный размер словаря

embedding:
  initialization: "uniform"       # Инициализация эмбеддингов
  init_range: 0.1                 # Диапазон инициализации
  freeze: false                   # Заморозка эмбеддингов
  pretrained: null                # Предобученные эмбеддинги

training:
  device: "cuda"                  # Устройство (cuda/cpu)
  batch_size: 32                  # Размер батча
  epochs: 10                      # Количество эпох
  learning_rate: 0.001            # Скорость обучения
  weight_decay: 1e-5              # L2 регуляризация
  patience: 3                     # Ранняя остановка
  gradient_clip: 1.0              # Клиппинг градиентов
  
  optimizer:
    type: "adam"
    parameters:
      lr: 0.001
      betas: [0.9, 0.999]
      eps: 1e-8
      weight_decay: 1e-5

  scheduler:
    type: "reduce_on_plateau"
    parameters:
      mode: "min"
      factor: 0.5
      patience: 2
      min_lr: 1e-6

  loss: "cross_entropy"           # Функция потерь

data_processing:
  max_length: 128                 # Максимальная длина последовательности
  padding: "post"                 # Паддинг в конце
  truncation: "post"              # Обрезка в конце
  lowercase: true                 # Приведение к нижнему регистру

evaluation:
  metrics: ["accuracy", "precision_macro", "recall_macro", "f1_macro"]
  batch_size: 32                  # Размер батча для оценки

# Параметры из статьи
article_parameters:
  embedding_dim: 200
  hidden_size: 128
  num_layers: 1
  bidirectional: true
  dropout: 0.3
  batch_size: 32
  epochs: 10
  learning_rate: 0.001
  max_length: 128
  patience: 3